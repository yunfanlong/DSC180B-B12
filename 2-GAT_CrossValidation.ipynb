{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "respiratory-pierce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yunfa\\AppData\\Local\\Temp\\ipykernel_30012\\3045016240.py:27: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:39: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30012\\3045016240.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    303\u001b[0m                 \u001b[1;32mfrom\u001b[0m \u001b[0mgat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGAT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHeteGAT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHeteGAT_multi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m                 \u001b[1;31m# Create the model with the current set of parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m                 \u001b[0mcr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2_coef\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhid_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_heads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mffd_drop_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_drop_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m                 \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m                 \u001b[0mcrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30012\\3045016240.py\u001b[0m in \u001b[0;36mHAN\u001b[1;34m(nb_epochs, l2_coef, lr, hid_units, n_heads, ffd_drop_set, attn_drop_set, fold, weight)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[0mtest_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[0mbiases_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madj_to_bias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnb_nodes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnhood\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0madj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0madj_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'input'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30012\\3045016240.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[0mtest_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[0mbiases_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madj_to_bias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnb_nodes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnhood\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0madj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0madj_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'input'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\yunfa\\dsc180b\\process.py\u001b[0m in \u001b[0;36madj_to_bias\u001b[1;34m(adj, sizes, nhood)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mmt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnhood\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mmt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0madj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "GAT Prediction Cross Validation\n",
    "\n",
    "Code Adapted from Alva Yan's Post-Covid Sentiment Analysis,\n",
    "https://github.com/AlvaYan/postCOVIDSentiAnalysis, and\n",
    "https://github.com/AlvaYan/Sentiment-Analysis-GNN-During-COVID19 with permission.\n",
    "\n",
    "Code was adapted while referencing public examples from the\n",
    "Keras documentation on GitHub:\n",
    "https://github.com/fchollet/keras/blob/master/examples\n",
    "'''\n",
    "\n",
    "#Cross validation. From a set of candidate parameters, see which parameter set performs the best s\n",
    "#make sure model code (gat, layers, base_gattn) is in path \"path_code\" \n",
    "#Code in this nodebook is modified from: https://github.com/Jhy1993/HAN \n",
    "\n",
    "path_result='C:\\\\Users\\\\yunfa\\\\dsc180b\\\\results\\\\'\n",
    "path_data=\"C:\\\\Users\\\\yunfa\\\\dsc180b\\\\data\"\n",
    "path_code=\"C:\\\\Users\\\\yunfa\\\\dsc180b\"\n",
    "path_temp=\"C:\\\\Users\\\\yunfa\\\\dsc180b\\\\temp\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "os.chdir(path_code)\n",
    "import gat\n",
    "import imp\n",
    "imp.reload(gat)\n",
    "from gat import GAT, HeteGAT, HeteGAT_multi  # or * for that matter\n",
    "import process\n",
    "import importlib\n",
    "importlib.reload(process)\n",
    "import numpy as np\n",
    "import pickle\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# jhy data\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def load_data_dblp(fold,path=None):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import scipy.sparse\n",
    "    from scipy.sparse import csc_matrix\n",
    "    from scipy import sparse\n",
    "    import numpy as np \n",
    "    import os\n",
    "    import csv \n",
    "    import numpy as np\n",
    "    # schools = [\"notredame\",\"uofm\",\"columbia\",\"dartmouth\",\"UCSD\",\"berkeley\",\"Harvard\",\"ucla\"]\n",
    "    schools = [\"notredame\",\"uofm\",\"columbia\",\"UCSD\",\"berkeley\",\"Harvard\",\"ucla\"]\n",
    "    s=schools[1]#use this index to control which network we are going to run.\n",
    "    t=\"2019\"\n",
    "    \n",
    "    os.chdir(path_temp)\n",
    "    \n",
    "    name=\"feature_\"+s+t+\".csv\"\n",
    "    truefeatures=pd.read_csv(name,skip_blank_lines=True,header=None).values\n",
    "    N=truefeatures.shape[0]\n",
    "    \n",
    "    os.chdir(path_data)\n",
    "    \n",
    "    name=\"label_emt3_\"+s+t+\"_cm.csv\"\n",
    "    truelabels=pd.read_csv(name,skip_blank_lines=True,header=None)\n",
    "    \n",
    "    name=\"CCasym_\"+s+t+\".npz\"\n",
    "    dat_cc=scipy.sparse.load_npz(name);dat_cc=dat_cc.toarray();\n",
    "    rownetworks = [(dat_cc)]\n",
    "    \n",
    "    #TODO: dartmouth change to 2019, missing 2020\n",
    "    os.chdir(path_data)\n",
    "\n",
    "    y=truelabels    \n",
    "    name=\"train_index_\"+s+t+\"_cm.p\"\n",
    "    train_idx = pickle.load(open(name,\"rb\"));train_idx=np.array(train_idx)\n",
    "    name=\"to_be_labeled_index_\"+s+t+\"_cm.p\"\n",
    "    val_idx = pickle.load(open(name,\"rb\"));val_idx=np.array(val_idx)\n",
    "    name=\"test_index_\"+s+t+\"_cm.p\"\n",
    "    #name=\"test_index_\"+s+y+\"_cm.p\"#use this to do 3 class CV\n",
    "    test_idx = pickle.load(open(name,\"rb\"));test_idx=np.array(test_idx)\n",
    "    import random\n",
    "    random.seed(3407)\n",
    "    random.shuffle(train_idx)\n",
    "    indexes=[]\n",
    "    temp=sample_mask(train_idx, y.shape[0])\n",
    "    tt=y.iloc[temp, :]\n",
    "    \n",
    "    t=np.sum(tt,axis=0)\n",
    "    n_neg=t[0]\n",
    "    n_pos=t[1]\n",
    "    train_idx=tuple(train_idx)\n",
    "    train_idx_tm=list(train_idx)\n",
    "    for k in range(kfolds):\n",
    "    \n",
    "        bgt_neg=round(n_neg/kfolds)\n",
    "        bgt_pos=round(n_pos/kfolds)\n",
    "        if k==kfolds-1:\n",
    "            bgt_neg=n_neg-k*round(n_neg/kfolds)\n",
    "            bgt_pos=n_pos-k*round(n_pos/kfolds)\n",
    "        group_index=[]\n",
    "        for train in train_idx:\n",
    "            if list(y.iloc[train,]).index(1)==0 and bgt_neg>0:\n",
    "                group_index.append(train)\n",
    "                train_idx_tm.remove(train)\n",
    "                bgt_neg-=1\n",
    "            elif list(y.iloc[train,]).index(1)==1 and bgt_pos>0:\n",
    "                group_index.append(train)\n",
    "                train_idx_tm.remove(train)\n",
    "                bgt_pos-=1\n",
    "        train_idx=tuple(train_idx_tm)\n",
    "        indexes.append(group_index)\n",
    "    \n",
    "    val_idx=np.array(indexes[fold])\n",
    "    import itertools\n",
    "    train_idx=list(itertools.chain.from_iterable(indexes))\n",
    "    for t in val_idx: train_idx.remove(t)\n",
    "    train_idx=np.array(train_idx)\n",
    "    \n",
    "    train_mask = sample_mask(train_idx, y.shape[0])\n",
    "    val_mask = sample_mask(val_idx, y.shape[0])\n",
    "    test_mask = sample_mask(test_idx, y.shape[0])\n",
    "    \n",
    "    y_train = np.zeros(y.shape)\n",
    "    y_val = np.zeros(y.shape)\n",
    "    y_test = np.zeros(y.shape)\n",
    "    y=y.values\n",
    "    y_train[train_mask, :] = y[train_mask, :]\n",
    "    y_val[val_mask, :] = y[val_mask, :]\n",
    "    y_test[test_mask, :] = y[test_mask, :]\n",
    "    truefeatures_list = [truefeatures, truefeatures]#, truefeatures, truefeatures]# ?? why copy three times? First for center node, 2 for each metapath.    \n",
    "    os.chdir(path_code)\n",
    "    return rownetworks, truefeatures_list, y_train, y_val, y_val, train_mask, val_mask, val_mask\n",
    "\n",
    "def HAN(nb_epochs= 100,l2_coef= 0.05,lr= 0.005,hid_units= [8],n_heads= [4, 1] ,ffd_drop_set=0,attn_drop_set=0,fold=0, weight=1):\n",
    "\n",
    "    dataset = 'acm'\n",
    "    featype = 'fea'\n",
    "    checkpt_file = path_result+'acm_allMP_multi_fea_2cls.ckpt'\n",
    "    batch_size = 1\n",
    "    patience = 100\n",
    "    residual = False\n",
    "    nonlinearity = tf.nn.elu\n",
    "    model = HeteGAT_multi\n",
    "\n",
    "    adj_list, fea_list, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data_dblp(fold=fold)\n",
    "    if featype == 'adj':\n",
    "        fea_list = adj_list\n",
    "    nb_nodes = fea_list[0].shape[0]\n",
    "    ft_size = fea_list[0].shape[1]\n",
    "    nb_classes = y_train.shape[1]\n",
    "\n",
    "    fea_list = [fea[np.newaxis] for fea in fea_list]\n",
    "    adj_list = [adj[np.newaxis] for adj in adj_list]\n",
    "    y_train = y_train[np.newaxis]\n",
    "    y_val = y_val[np.newaxis]\n",
    "    y_test = y_test[np.newaxis]\n",
    "    train_mask = train_mask[np.newaxis]\n",
    "    val_mask = val_mask[np.newaxis]\n",
    "    test_mask = test_mask[np.newaxis]\n",
    "\n",
    "    biases_list = [process.adj_to_bias(adj, [nb_nodes], nhood=1) for adj in adj_list]\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.name_scope('input'):\n",
    "            ftr_in_list = [tf.placeholder(dtype=tf.float32,\n",
    "                                          shape=(batch_size, nb_nodes, ft_size),\n",
    "                                          name='ftr_in_{}'.format(i))\n",
    "                           for i in range(len(fea_list))]\n",
    "            bias_in_list = [tf.placeholder(dtype=tf.float32,\n",
    "                                           shape=(batch_size, nb_nodes, nb_nodes),\n",
    "                                           name='bias_in_{}'.format(i))\n",
    "                            for i in range(len(biases_list))]\n",
    "            lbl_in = tf.placeholder(dtype=tf.int32, shape=(\n",
    "                batch_size, nb_nodes, nb_classes), name='lbl_in')\n",
    "            msk_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes),\n",
    "                                    name='msk_in')\n",
    "            attn_drop = tf.placeholder(dtype=tf.float32, shape=(), name='attn_drop')\n",
    "            ffd_drop = tf.placeholder(dtype=tf.float32, shape=(), name='ffd_drop')\n",
    "            is_train = tf.placeholder(dtype=tf.bool, shape=(), name='is_train')\n",
    "        # forward\n",
    "        logits, final_embedding, att_val = model.inference(ftr_in_list, nb_classes, nb_nodes, is_train,\n",
    "                                                       attn_drop, ffd_drop,\n",
    "                                                       bias_mat_list=bias_in_list,\n",
    "                                                       hid_units=hid_units, n_heads=n_heads,\n",
    "                                                       residual=residual, activation=nonlinearity)\n",
    "\n",
    "        # cal masked_loss\n",
    "        log_resh = tf.reshape(logits, [-1, nb_classes])\n",
    "        lab_resh = tf.reshape(lbl_in, [-1, nb_classes])\n",
    "        msk_resh = tf.reshape(msk_in, [-1])\n",
    "        loss = model.masked_softmax_cross_entropy(log_resh, lab_resh, msk_resh, weight)\n",
    "        predicted=tf.argmax(log_resh, 1)\n",
    "        accuracy = model.masked_accuracy(log_resh, lab_resh, msk_resh)\n",
    "        # optimzie\n",
    "        train_op = model.training(loss, lr, l2_coef)\n",
    "    \n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        init_op = tf.group(tf.global_variables_initializer(),\n",
    "                       tf.local_variables_initializer())\n",
    "\n",
    "        vlss_mn = np.inf\n",
    "        vacc_mx = 0.0\n",
    "        curr_step = 0\n",
    "\n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(init_op)\n",
    "\n",
    "            train_loss_avg = 0\n",
    "            train_acc_avg = 0\n",
    "            val_loss_avg = 0\n",
    "            val_acc_avg = 0\n",
    "            val_pred_avg = 0\n",
    "            for epoch in range(nb_epochs):\n",
    "                tr_step = 0\n",
    "           \n",
    "                tr_size = fea_list[0].shape[0]\n",
    "                # ================   training    ============\n",
    "                while tr_step * batch_size < tr_size:\n",
    "\n",
    "                    fd1 = {i: d[tr_step * batch_size:(tr_step + 1) * batch_size]\n",
    "                           for i, d in zip(ftr_in_list, fea_list)}\n",
    "                    fd2 = {i: d[tr_step * batch_size:(tr_step + 1) * batch_size]\n",
    "                           for i, d in zip(bias_in_list, biases_list)}\n",
    "                    fd3 = {lbl_in: y_train[tr_step * batch_size:(tr_step + 1) * batch_size],\n",
    "                       msk_in: train_mask[tr_step * batch_size:(tr_step + 1) * batch_size],\n",
    "                       is_train: True,\n",
    "                       attn_drop: attn_drop_set,\n",
    "                       ffd_drop: ffd_drop_set}\n",
    "                    fd = fd1\n",
    "                    fd.update(fd2)\n",
    "                    fd.update(fd3)\n",
    "                    _, loss_value_tr, acc_tr, att_val_train = sess.run([train_op, loss, accuracy, att_val],\n",
    "                                                                   feed_dict=fd)\n",
    "                    train_loss_avg += loss_value_tr\n",
    "                    train_acc_avg += acc_tr\n",
    "                    tr_step += 1\n",
    "\n",
    "                vl_step = 0\n",
    "                vl_size = fea_list[0].shape[0]\n",
    "                curr_step += 1\n",
    "                if epoch==nb_epochs-1:\n",
    "                    saver.save(sess, checkpt_file)\n",
    "\n",
    "                train_loss_avg = 0\n",
    "                train_acc_avg = 0\n",
    "                val_loss_avg = 0\n",
    "                val_acc_avg = 0\n",
    "            saver.restore(sess, checkpt_file)\n",
    "            ts_size = fea_list[0].shape[0]\n",
    "            ts_step = 0\n",
    "            ts_loss = 0.0\n",
    "            ts_acc = 0.0\n",
    "\n",
    "            while ts_step * batch_size < ts_size:\n",
    "                fd1 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                       for i, d in zip(ftr_in_list, fea_list)}\n",
    "                fd2 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                       for i, d in zip(bias_in_list, biases_list)}\n",
    "                fd3 = {lbl_in: y_test[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "                       msk_in: test_mask[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "            \n",
    "                       is_train: False,\n",
    "                       attn_drop: attn_drop_set,\n",
    "                       ffd_drop: ffd_drop_set}\n",
    "        \n",
    "                fd = fd1\n",
    "                fd.update(fd2)\n",
    "                fd.update(fd3)\n",
    "                loss_value_ts, acc_ts, jhy_final_embedding = sess.run([loss, accuracy, final_embedding],\n",
    "                                                                  feed_dict=fd)\n",
    "                ts_loss += loss_value_ts\n",
    "                ts_acc += acc_ts\n",
    "                ts_step += 1\n",
    "\n",
    "            print('Test loss:', ts_loss / ts_step,\n",
    "              '; Test accuracy:', ts_acc / ts_step)\n",
    "            \n",
    "            #crs.append(ts_loss / ts_step)\n",
    "            #crs=tuple(crs)\n",
    "            import pickle\n",
    "            \n",
    "\n",
    "            sess.close()\n",
    "            return(ts_loss / ts_step)\n",
    "\n",
    "# l2_candi=list([0.02, 0.05, 0.08,0.1,0.2])\n",
    "l2_candi=list([0.08])\n",
    "lr_candi=list([0.02])\n",
    "weight_candi=list([2])\n",
    "\n",
    "kfolds=2\n",
    "\n",
    "records=[]\n",
    "count=0\n",
    "\n",
    "for p1 in l2_candi:\n",
    "    for p2 in lr_candi:\n",
    "        for p3 in weight_candi:\n",
    "            crs = []\n",
    "            for fold in range(kfolds):\n",
    "                imp.reload(gat)\n",
    "                from gat import GAT, HeteGAT, HeteGAT_multi\n",
    "                # Create the model with the current set of parameters\n",
    "                cr = HAN(nb_epochs=100, l2_coef=p1, lr=p2, hid_units=[8], n_heads=[4, 1], ffd_drop_set=0, attn_drop_set=0, fold=fold, weight=p3)\n",
    "                count += 1\n",
    "                crs.append(cr)\n",
    "            \n",
    "            # Store the parameters and their corresponding results\n",
    "            records.append({\n",
    "                'params': {\n",
    "                    'l2_coef': p1,\n",
    "                    'lr': p2,\n",
    "                    'weight': p3\n",
    "                },\n",
    "                'results': crs\n",
    "            })\n",
    "            print(f'Finished running parameter set: l2_coef={p1}, lr={p2}, weight={p3} @_@')\n",
    "            \n",
    "print(f'Total models trained: {count}')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb70caf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf2172a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu==1.15\n",
      "  Downloading tensorflow_gpu-1.15.0-cp37-cp37m-win_amd64.whl (294.5 MB)\n",
      "     -------------------------------------- 294.5/294.5 MB 5.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in c:\\users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-gpu==1.15) (1.0.8)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-gpu==1.15) (0.8.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in c:\\users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-gpu==1.15) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-gpu==1.15) (1.21.5)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in c:\\users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-gpu==1.15) (3.20.3)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-gpu==1.15) (0.15.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-gpu==1.15) (3.3.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-gpu==1.15) (1.16.0)\n",
      "Collecting tensorboard<1.16.0,>=1.15.0\n",
      "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
      "     ---------------------------------------- 3.8/3.8 MB 24.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-gpu==1.15) (1.14.1)\n",
      "Requirement already satisfied: gast==0.2.2 in c:\\users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-gpu==1.15) (0.2.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-gpu==1.15) (1.1.2)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-gpu==1.15) (0.38.4)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-gpu==1.15) (1.1.0)\n",
      "Collecting tensorflow-estimator==1.15.1\n",
      "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
      "     ------------------------------------- 503.4/503.4 kB 30.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-gpu==1.15) (1.42.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (3.7.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (0.16.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (65.6.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (4.4.0)\n",
      "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow-gpu\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.6.0\n",
      "    Uninstalling tensorflow-estimator-2.6.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.8.0\n",
      "    Uninstalling tensorboard-2.8.0:\n",
      "      Successfully uninstalled tensorboard-2.8.0\n",
      "Successfully installed tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-gpu==1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce887b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"tensorflow>=1.15,<2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-dylan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy paste print-outs from last block to \"xvali.txt\" in your \"path_result\" and save it before running this block!\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "path_result='C:\\\\Users\\\\yunfa\\\\180b\\\\results\\\\'\n",
    "\n",
    "os.chdir(path_result)\n",
    "\n",
    "file1 = open('xvali.txt', 'r')\n",
    "Lines = file1.readlines()\n",
    "new_lines=[a.replace('\\n','').replace('[','').replace(']','').replace(',','') for a in Lines if a!='\\n']\n",
    "print(len(new_lines))\n",
    "new_lines1=[]\n",
    "for a in range(len(new_lines)):\n",
    "    if new_lines[a][0:4]=='Test':\n",
    "        new_lines1.append(new_lines[a])\n",
    "new_lines2=[]\n",
    "print(len(new_lines1))\n",
    "for a in range(len(new_lines1)):\n",
    "    acc=float(new_lines1[a].split('Test accuracy: ')[1])\n",
    "    new_lines2.append(acc)\n",
    "print(len(new_lines2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See which parameter combination perform the best\n",
    "records_agg=[]\n",
    "\n",
    "#repeats=10\n",
    "#kfolds=4\n",
    "for a1 in range(len(l2_candi)):\n",
    "    r1=[]\n",
    "    for a2 in range(len(lr_candi)):\n",
    "        r2=[]\n",
    "        for a3 in range(len(weight_candi)):\n",
    "            r3=[]\n",
    "            for a4 in range(repeats):\n",
    "                r4=[]\n",
    "                for a5 in range(kfolds): \n",
    "                    r4.append(new_lines2[0])\n",
    "                    new_lines2.pop(0)\n",
    "                r3.append(r4)\n",
    "            r2.append(r3)\n",
    "        r1.append(r2)\n",
    "    records_agg.append(r1)\n",
    "\n",
    "\n",
    "r=np.array(records_agg,dtype='float')\n",
    "print(r.shape)\n",
    "#print(r)\n",
    "w=np.mean(r,axis=3)\n",
    "print(w.shape)\n",
    "e=np.mean(w,axis=3)\n",
    "print(e.shape)\n",
    "pos=np.where(e== e.max())\n",
    "print(e)\n",
    "print(pos)\n",
    "\n",
    "print('l2:'+str(l2_candi[pos[0][0]]))\n",
    "print('lr:'+str(lr_candi[pos[1][0]]))\n",
    "print('weight:'+str(weight_candi[pos[2][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weight_candi)\n",
    "print((lr_candi))\n",
    "print(l2_candi)\n",
    "\n",
    "print(repeats)\n",
    "print(kfolds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
