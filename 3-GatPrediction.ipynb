{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dense-appeal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: C:\\Users\\yunfa\\dsc180b\\resultsacm_allMP_multi_fea_.ckpt\n",
      "Dataset: acm\n",
      "----- Opt. hyperparams -----\n",
      "lr: 0.002\n",
      "l2_coef: 0.08\n",
      "----- Archi. hyperparams -----\n",
      "nb. layers: 1\n",
      "nb. units per layer: [8]\n",
      "nb. attention heads: [4, 1]\n",
      "residual: False\n",
      "nonlinearity: <function elu at 0x000002948FB11C18>\n",
      "model: <class 'gat.HeteGAT_multi'>\n",
      "81 12524\n",
      "y_train:(12524, 2), y_val:(12524, 2), y_test:(12524, 2), train_idx:(81,), val_idx:(12343,), test_idx:(100,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:71: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26412\\2591178831.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[0mtest_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m \u001b[0mbiases_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madj_to_bias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnb_nodes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnhood\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0madj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0madj_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;31m#biases_list = [process.adj_to_bias(adj) for adj in adj_list]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;31m#process.adj_to_bias(adj)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26412\\2591178831.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[0mtest_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m \u001b[0mbiases_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madj_to_bias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnb_nodes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnhood\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0madj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0madj_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;31m#biases_list = [process.adj_to_bias(adj) for adj in adj_list]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;31m#process.adj_to_bias(adj)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\yunfa\\dsc180b\\process.py\u001b[0m in \u001b[0;36madj_to_bias\u001b[1;34m(adj, sizes, nhood)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mmt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m                     \u001b[0mmt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1e9\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "GAT Prediction Classifier\n",
    "\n",
    "Code Adapted from Alva Yan's Post-Covid Sentiment Analysis,\n",
    "https://github.com/AlvaYan/postCOVIDSentiAnalysis, and\n",
    "https://github.com/AlvaYan/Sentiment-Analysis-GNN-During-COVID19 with permission.\n",
    "\n",
    "Code was adapted while referencing public examples from the\n",
    "Keras documentation on GitHub:\n",
    "https://github.com/fchollet/keras/blob/master/examples\n",
    "'''\n",
    "\n",
    "#Get the model\n",
    "path_result='C:\\\\Users\\\\yunfa\\\\dsc180b\\\\results'\n",
    "path_data=\"C:\\\\Users\\\\yunfa\\\\dsc180b\\\\data\"\n",
    "path_code=\"C:\\\\Users\\\\yunfa\\\\dsc180b\"\n",
    "path_temp=\"C:\\\\Users\\\\yunfa\\\\dsc180b\\\\temp\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "os.chdir(path_code)\n",
    "import gat\n",
    "import imp\n",
    "imp.reload(gat)\n",
    "from gat import GAT, HeteGAT, HeteGAT_multi  # or * for that matter\n",
    "import process\n",
    "import importlib\n",
    "importlib.reload(process)\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy.sparse\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy import sparse\n",
    "import csv \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "dataset = 'acm'\n",
    "featype = 'fea'\n",
    "checkpt_file=path_result+'acm_allMP_multi_fea_.ckpt'\n",
    "\n",
    "print('model: {}'.format(checkpt_file))\n",
    "# training params for only CC\n",
    "batch_size = 1\n",
    "nb_epochs = 30\n",
    "patience = 100\n",
    "lr = 0.002  # learning rate\n",
    "l2_coef = 0.08#0.1#0.001  # weight decay\n",
    "# numbers of hidden units per each attention head in each layer\n",
    "hid_units = [8]\n",
    "n_heads = [4, 1]  # additional entry for the output layer\n",
    "residual = False\n",
    "nonlinearity = tf.nn.elu\n",
    "model = HeteGAT_multi\n",
    "\n",
    "print('Dataset: ' + dataset)\n",
    "print('----- Opt. hyperparams -----')\n",
    "print('lr: ' + str(lr))\n",
    "print('l2_coef: ' + str(l2_coef))\n",
    "print('----- Archi. hyperparams -----')\n",
    "print('nb. layers: ' + str(len(hid_units)))\n",
    "print('nb. units per layer: ' + str(hid_units))\n",
    "print('nb. attention heads: ' + str(n_heads))\n",
    "print('residual: ' + str(residual))\n",
    "print('nonlinearity: ' + str(nonlinearity))\n",
    "print('model: ' + str(model))\n",
    "\n",
    "# jhy data\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1 #[0, 0, 0...1(53)... 0(1274)]\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def load_data_dblp(path=path_data):\n",
    "    schools = [\"notredame\",\"uofm\",\"columbia\",\"dartmouth\",\"UCSD\",\"berkeley\",\"Harvard\",\"ucla\"]\n",
    "    s=schools[1]#use this index to control which network we are going to run.\n",
    "    t=\"2019\"\n",
    "    \n",
    "    os.chdir(path_data)\n",
    "    name=\"label_emt3_\"+s+t+\"_cm.csv\"\n",
    "    truelabels=pd.read_csv(name,skip_blank_lines=True,header=None)\n",
    "    \n",
    "    os.chdir(path_temp)\n",
    "    name=\"feature_\"+s+t+\".csv\"\n",
    "    truefeatures=pd.read_csv(name,skip_blank_lines=True,header=None).values\n",
    "    N=truefeatures.shape[0]\n",
    "    \n",
    "    os.chdir(path_data)\n",
    "    name=\"CCsym_\"+s+t+\".npz\"\n",
    "    dat_cc=scipy.sparse.load_npz(name);dat_cc=dat_cc.toarray()\n",
    "    rownetworks = [(dat_cc)]\n",
    "    \n",
    "    y=truelabels\n",
    "    name=\"train_index_\"+s+t+\"_cm.p\"\n",
    "    train_idx = pickle.load(open(name,\"rb\"));train_idx=np.array(train_idx)\n",
    "    name=\"to_be_labeled_index_\"+s+t+\"_cm.p\"\n",
    "    val_idx = pickle.load(open(name,\"rb\"));val_idx=np.array(val_idx)\n",
    "    name=\"test_index_\"+s+t+\"_cm.p\"\n",
    "    test_idx = pickle.load(open(name,\"rb\"));test_idx=np.array(test_idx)\n",
    "    \n",
    "    print(train_idx.shape[0], y.shape[0])\n",
    "\n",
    "    train_mask = sample_mask(train_idx, y.shape[0])\n",
    "    val_mask = sample_mask(val_idx, y.shape[0])\n",
    "    test_mask = sample_mask(test_idx, y.shape[0])\n",
    "    \n",
    "    y_train = np.zeros(y.shape)\n",
    "    y_val = np.zeros(y.shape)\n",
    "    y_test = np.zeros(y.shape)\n",
    "    y=y.values\n",
    "    y_train[train_mask, :] = y[train_mask, :]\n",
    "    y_val[val_mask, :] = y[val_mask, :]\n",
    "    y_test[test_mask, :] = y[test_mask, :]\n",
    "\n",
    "    # return selected_idx, selected_idx_2\n",
    "    print('y_train:{}, y_val:{}, y_test:{}, train_idx:{}, val_idx:{}, test_idx:{}'.format(y_train.shape,\n",
    "                                                                                          y_val.shape,\n",
    "                                                                                          y_test.shape,\n",
    "                                                                                          train_idx.shape,\n",
    "                                                                                          val_idx.shape,\n",
    "                                                                                          test_idx.shape))\n",
    "    truefeatures_list = [truefeatures, truefeatures]#, truefeatures, truefeatures]# ?? why copy three times? First for center node, 2 for each metapath.    \n",
    "    os.chdir(path_code)\n",
    "    return rownetworks, truefeatures_list, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "# use adj_list as fea_list, have a try~\n",
    "adj_list, fea_list, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data_dblp()\n",
    "if featype == 'adj':\n",
    "    fea_list = adj_list\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "nb_nodes = fea_list[0].shape[0]\n",
    "ft_size = fea_list[0].shape[1]\n",
    "nb_classes = y_train.shape[1]\n",
    "\n",
    "fea_list = [fea[np.newaxis] for fea in fea_list]\n",
    "adj_list = [adj[np.newaxis] for adj in adj_list]\n",
    "y_train = y_train[np.newaxis]\n",
    "y_val = y_val[np.newaxis]\n",
    "y_test = y_test[np.newaxis]\n",
    "train_mask = train_mask[np.newaxis]\n",
    "val_mask = val_mask[np.newaxis]\n",
    "test_mask = test_mask[np.newaxis]\n",
    "\n",
    "biases_list = [process.adj_to_bias(adj, [nb_nodes], nhood=1) for adj in adj_list]\n",
    "#biases_list = [process.adj_to_bias(adj) for adj in adj_list]\n",
    "#process.adj_to_bias(adj)\n",
    "print('build graph...')\n",
    "with tf.Graph().as_default():\n",
    "    with tf.name_scope('input'):\n",
    "        ftr_in_list = [tf.placeholder(dtype=tf.float32,\n",
    "                                      shape=(batch_size, nb_nodes, ft_size),\n",
    "                                      name='ftr_in_{}'.format(i))\n",
    "                       for i in range(len(fea_list))]\n",
    "        bias_in_list = [tf.placeholder(dtype=tf.float32,\n",
    "                                       shape=(batch_size, nb_nodes, nb_nodes),\n",
    "                                       name='bias_in_{}'.format(i))\n",
    "                        for i in range(len(biases_list))]\n",
    "        lbl_in = tf.placeholder(dtype=tf.int32, shape=(\n",
    "            batch_size, nb_nodes, nb_classes), name='lbl_in')\n",
    "        msk_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes),\n",
    "                                name='msk_in')\n",
    "        attn_drop = tf.placeholder(dtype=tf.float32, shape=(), name='attn_drop')\n",
    "        ffd_drop = tf.placeholder(dtype=tf.float32, shape=(), name='ffd_drop')\n",
    "        is_train = tf.placeholder(dtype=tf.bool, shape=(), name='is_train')\n",
    "    # forward\n",
    "    logits, final_embedding, att_val = model.inference(ftr_in_list, nb_classes, nb_nodes, is_train,\n",
    "                                                       attn_drop, ffd_drop,\n",
    "                                                       bias_mat_list=bias_in_list,\n",
    "                                                       hid_units=hid_units, n_heads=n_heads,\n",
    "                                                       residual=residual, activation=nonlinearity)\n",
    "\n",
    "    # cal masked_loss\n",
    "    log_resh = tf.reshape(logits, [-1, nb_classes])\n",
    "    lab_resh = tf.reshape(lbl_in, [-1, nb_classes])\n",
    "    msk_resh = tf.reshape(msk_in, [-1])\n",
    "    loss = model.masked_softmax_cross_entropy(log_resh, lab_resh, msk_resh)\n",
    "    predicted=tf.argmax(log_resh, 1)\n",
    "    accuracy = model.masked_accuracy(log_resh, lab_resh, msk_resh)\n",
    "    # optimzie\n",
    "    train_op = model.training(loss, lr, l2_coef)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                       tf.local_variables_initializer())\n",
    "\n",
    "    vlss_mn = np.inf\n",
    "    vacc_mx = 0.0\n",
    "    curr_step = 0\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init_op)\n",
    "        train_loss_avg = 0\n",
    "        train_acc_avg = 0\n",
    "        val_loss_avg = 0\n",
    "        val_acc_avg = 0\n",
    "        val_pred_avg = 0\n",
    "        for epoch in range(nb_epochs):\n",
    "            tr_step = 0\n",
    "           \n",
    "            tr_size = fea_list[0].shape[0]\n",
    "            # ================   training    ============\n",
    "            while tr_step * batch_size < tr_size:\n",
    "\n",
    "                fd1 = {i: d[tr_step * batch_size:(tr_step + 1) * batch_size]\n",
    "                       for i, d in zip(ftr_in_list, fea_list)}\n",
    "                fd2 = {i: d[tr_step * batch_size:(tr_step + 1) * batch_size]\n",
    "                       for i, d in zip(bias_in_list, biases_list)}\n",
    "                fd3 = {lbl_in: y_train[tr_step * batch_size:(tr_step + 1) * batch_size],\n",
    "                       msk_in: train_mask[tr_step * batch_size:(tr_step + 1) * batch_size],\n",
    "                       is_train: True,\n",
    "                       attn_drop: 0,\n",
    "                       ffd_drop: 0}\n",
    "                fd = fd1\n",
    "                fd.update(fd2)\n",
    "                fd.update(fd3)\n",
    "                _, loss_value_tr, acc_tr, att_val_train = sess.run([train_op, loss, accuracy, att_val],\n",
    "                                                                   feed_dict=fd)\n",
    "                train_loss_avg += loss_value_tr\n",
    "                train_acc_avg += acc_tr\n",
    "                tr_step += 1\n",
    "\n",
    "            vl_step = 0\n",
    "            vl_size = fea_list[0].shape[0]\n",
    "\n",
    "            # import pdb; pdb.set_trace()\n",
    "            print('Epoch: {}, att_val: {}'.format(epoch, np.mean(att_val_train, axis=0)))\n",
    "            print('Training: loss = %.5f, acc = %.5f |' %\n",
    "                  (train_loss_avg / tr_step, train_acc_avg / tr_step))\n",
    "\n",
    "            curr_step += 1\n",
    "            if epoch==nb_epochs-1:\n",
    "                saver.save(sess, checkpt_file)\n",
    "\n",
    "            train_loss_avg = 0\n",
    "            train_acc_avg = 0\n",
    "            val_loss_avg = 0\n",
    "            val_acc_avg = 0\n",
    "        saver.restore(sess, checkpt_file)\n",
    "        print('load model from : {}'.format(checkpt_file))\n",
    "        ts_size = fea_list[0].shape[0]\n",
    "        ts_step = 0\n",
    "        ts_loss = 0.0\n",
    "        ts_acc = 0.0\n",
    "\n",
    "        while ts_step * batch_size < ts_size:\n",
    "            fd1 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                   for i, d in zip(ftr_in_list, fea_list)}\n",
    "            fd2 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                   for i, d in zip(bias_in_list, biases_list)}\n",
    "            fd3 = {lbl_in: y_test[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "                   msk_in: test_mask[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "            \n",
    "                   is_train: False,\n",
    "                   attn_drop: 0.0,\n",
    "                   ffd_drop: 0.0}\n",
    "        \n",
    "            fd = fd1\n",
    "            fd.update(fd2)\n",
    "            fd.update(fd3)\n",
    "            loss_value_ts, acc_ts, jhy_final_embedding = sess.run([loss, accuracy, final_embedding],\n",
    "                                                                  feed_dict=fd)\n",
    "            ts_loss += loss_value_ts\n",
    "            ts_acc += acc_ts\n",
    "            ts_step += 1\n",
    "\n",
    "        print('Test loss:', ts_loss / ts_step,\n",
    "              '; Test accuracy:', ts_acc / ts_step)\n",
    "        \n",
    "        print('start knn, kmean.....')\n",
    "        xx = np.expand_dims(jhy_final_embedding, axis=0)[test_mask]\n",
    "\n",
    "        from numpy import linalg as LA\n",
    "\n",
    "        yy = y_test[test_mask]\n",
    "\n",
    "        print('xx: {}, yy: {}'.format(xx.shape, yy.shape))\n",
    "        from jhyexp import my_KNN, my_Kmeans#, my_TSNE, my_Linear\n",
    "\n",
    "        my_KNN(xx, yy)\n",
    "        my_Kmeans(xx, yy)\n",
    "\n",
    "        sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "middle-therapist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: acm\n",
      "----- Opt. hyperparams -----\n",
      "100\n",
      "test_index_notredame2020_cm.p\n",
      "test_idx [   0    1    2 ... 7947 7948 7949]\n",
      "y.shape[0] 7950\n",
      "y_train:(7950, 2), y_val:(7950, 2), y_test:(7950, 2), train_idx:(104,), val_idx:(7746,), test_idx:(7950,)\n",
      "s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yunfa\\dsc180b\\resultsacm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 7950, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.03520938381552696 ; Test accuracy: 0.7075471878051758\n",
      "100\n",
      "test_index_uofm2019_cm.p\n",
      "test_idx [    0     1     2 ... 12521 12522 12523]\n",
      "y.shape[0] 12524\n",
      "y_train:(12524, 2), y_val:(12524, 2), y_test:(12524, 2), train_idx:(81,), val_idx:(12343,), test_idx:(12524,)\n",
      "s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yunfa\\dsc180b\\resultsacm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 12524, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.017942721024155617 ; Test accuracy: 0.6664004921913147\n",
      "100\n",
      "test_index_uofm2020_cm.p\n",
      "test_idx [    0     1     2 ... 14876 14877 14878]\n",
      "y.shape[0] 14879\n",
      "y_train:(14879, 2), y_val:(14879, 2), y_test:(14879, 2), train_idx:(68,), val_idx:(14711,), test_idx:(14879,)\n",
      "s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yunfa\\dsc180b\\resultsacm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 14879, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.017300335690379143 ; Test accuracy: 0.7076416611671448\n",
      "100\n",
      "test_index_columbia2019_cm.p\n",
      "test_idx [   0    1    2 ... 5028 5029 5030]\n",
      "y.shape[0] 5031\n",
      "y_train:(5031, 2), y_val:(5031, 2), y_test:(5031, 2), train_idx:(42,), val_idx:(4889,), test_idx:(5031,)\n",
      "s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yunfa\\dsc180b\\resultsacm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 5031, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.04166422784328461 ; Test accuracy: 0.6491751074790955\n",
      "100\n",
      "test_index_columbia2020_cm.p\n",
      "test_idx [   0    1    2 ... 9334 9335 9336]\n",
      "y.shape[0] 9337\n",
      "y_train:(9337, 2), y_val:(9337, 2), y_test:(9337, 2), train_idx:(38,), val_idx:(9199,), test_idx:(9337,)\n",
      "s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yunfa\\dsc180b\\resultsacm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 9337, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.022579364478588104 ; Test accuracy: 0.6698082685470581\n",
      "100\n",
      "test_index_UCSD2019_cm.p\n",
      "test_idx [    0     1     2 ... 17499 17500 17501]\n",
      "y.shape[0] 17502\n",
      "y_train:(17502, 2), y_val:(17502, 2), y_test:(17502, 2), train_idx:(46,), val_idx:(17356,), test_idx:(17502,)\n",
      "s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yunfa\\dsc180b\\resultsacm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 17502, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.012491380795836449 ; Test accuracy: 0.6775797009468079\n",
      "100\n",
      "test_index_UCSD2020_cm.p\n",
      "test_idx [    0     1     2 ... 17343 17344 17345]\n",
      "y.shape[0] 17346\n",
      "y_train:(17346, 2), y_val:(17346, 2), y_test:(17346, 2), train_idx:(49,), val_idx:(17197,), test_idx:(17346,)\n",
      "s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yunfa\\dsc180b\\resultsacm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 17346, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.013872827403247356 ; Test accuracy: 0.6751989126205444\n",
      "100\n",
      "test_index_berkeley2019_cm.p\n",
      "test_idx [    0     1     2 ... 16670 16671 16672]\n",
      "y.shape[0] 16673\n",
      "y_train:(16673, 2), y_val:(16673, 2), y_test:(16673, 2), train_idx:(45,), val_idx:(16528,), test_idx:(16673,)\n",
      "s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yunfa\\dsc180b\\resultsacm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 16673, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.014486872591078281 ; Test accuracy: 0.666526734828949\n",
      "100\n",
      "test_index_berkeley2020_cm.p\n",
      "test_idx [    0     1     2 ... 18191 18192 18193]\n",
      "y.shape[0] 18194\n",
      "y_train:(18194, 2), y_val:(18194, 2), y_test:(18194, 2), train_idx:(30,), val_idx:(18064,), test_idx:(18194,)\n",
      "s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yunfa\\dsc180b\\resultsacm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 18194, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.013726333156228065 ; Test accuracy: 0.6751126646995544\n",
      "100\n",
      "test_index_Harvard2019_cm.p\n",
      "test_idx [   0    1    2 ... 2890 2891 2892]\n",
      "y.shape[0] 2893\n",
      "y_train:(2893, 2), y_val:(2893, 2), y_test:(2893, 2), train_idx:(46,), val_idx:(2747,), test_idx:(2893,)\n",
      "s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yunfa\\dsc180b\\resultsacm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 2893, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.08223983645439148 ; Test accuracy: 0.686138927936554\n",
      "100\n",
      "test_index_Harvard2020_cm.p\n",
      "test_idx [   0    1    2 ... 3551 3552 3553]\n",
      "y.shape[0] 3554\n",
      "y_train:(3554, 2), y_val:(3554, 2), y_test:(3554, 2), train_idx:(71,), val_idx:(3383,), test_idx:(3554,)\n",
      "s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yunfa\\dsc180b\\resultsacm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 3554, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.06663090735673904 ; Test accuracy: 0.6809229254722595\n",
      "100\n",
      "test_index_ucla2019_cm.p\n",
      "test_idx [    0     1     2 ... 16891 16892 16893]\n",
      "y.shape[0] 16894\n",
      "y_train:(16894, 2), y_val:(16894, 2), y_test:(16894, 2), train_idx:(37,), val_idx:(16757,), test_idx:(16894,)\n",
      "s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yunfa\\dsc180b\\resultsacm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 16894, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.015910383313894272 ; Test accuracy: 0.6646146774291992\n",
      "100\n",
      "test_index_ucla2020_cm.p\n",
      "test_idx [    0     1     2 ... 18711 18712 18713]\n",
      "y.shape[0] 18714\n",
      "y_train:(18714, 2), y_val:(18714, 2), y_test:(18714, 2), train_idx:(32,), val_idx:(18582,), test_idx:(18714,)\n",
      "s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yunfa\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yunfa\\dsc180b\\resultsacm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 18714, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.013245043344795704 ; Test accuracy: 0.7125681042671204\n"
     ]
    }
   ],
   "source": [
    "#run previous block before running this one\n",
    "#get the testing result without submission data, i.e., the graph that this block used only contains comments data\n",
    "#the printed output is meaningless\n",
    "#predicted results are saved to .p file\n",
    "#prediction in this step is used in step 4, but not in step 5\n",
    "#if you have memory error, calculate \"biases_list\" variable for each school and save it locally, then jsut load during running. The code for this is commented out and you can use it directly\n",
    "\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.chdir(path_code)\n",
    "from gat import GAT, HeteGAT, HeteGAT_multi\n",
    "import process\n",
    "import importlib\n",
    "from layers import attn_head, SimpleAttLayer\n",
    "importlib.reload(process)\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy.sparse\n",
    "from scipy.sparse import csc_matrix\n",
    "# import scipy import sparse\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import csv \n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "dataset = 'acm'\n",
    "featype = 'fea'\n",
    "print('Dataset: ' + dataset)\n",
    "print('----- Opt. hyperparams -----')\n",
    "\n",
    "# jhy data\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "#TODO: removed \"notredame2019\", \"dartmouth2019\", \"dartmouth2020\"\n",
    "schools = [\"notredame2020\",\"uofm2019\",\"uofm2020\",\"columbia2019\",\"columbia2020\",\"UCSD2019\",\"UCSD2020\",\"berkeley2019\",\"berkeley2020\",\"Harvard2019\",\"Harvard2020\",\"ucla2019\",\"ucla2020\"]\n",
    "schools_type = [\"notredame2020\",\"uofm2019\",\"uofm2020\",\"columbia2019\",\"columbia2020\",\"UCSD2019\",\"UCSD2020\",\n",
    "           \"berkeley2019\",\"berkeley2020\",\"Harvard2019\",\"Harvard2020\",\"ucla2019\",\"ucla2020\"]\n",
    "\n",
    "def load_data_dblp(s,st,path=path_data):\n",
    "    # t=\"comment\"\n",
    "    \n",
    "    os.chdir(path_data)\n",
    "    name=\"label_emt3_\"+s+\"_cm.csv\"\n",
    "    truelabels=pd.read_csv(name,skip_blank_lines=True,header=None)\n",
    "    \n",
    "    os.chdir(path_temp)\n",
    "    name=\"feature_\"+s+\".csv\"\n",
    "    truefeatures=pd.read_csv(name,skip_blank_lines=True,header=None).values\n",
    "\n",
    "    os.chdir(path_data)\n",
    "    N=truefeatures.shape[0]\n",
    "    name=\"CCsym_\"+s+\".npz\"\n",
    "    dat_cc=scipy.sparse.load_npz(name);dat_cc=dat_cc.toarray();\n",
    "    rownetworks = [(dat_cc)]\n",
    "\n",
    "    y=truelabels\n",
    "    name=\"train_index_\"+s+\"_cm.p\"\n",
    "    train_idx = pickle.load(open(name,\"rb\"));train_idx=np.array(train_idx)\n",
    "    name=\"to_be_labeled_index_\"+s+\"_cm.p\"\n",
    "    val_idx = pickle.load(open(name,\"rb\"));val_idx=np.array(val_idx)\n",
    "    name=\"test_index_\"+s+\"_cm.p\"\n",
    "    test_idx = pickle.load(open(name,\"rb\"));test_idx=np.array(test_idx)\n",
    "    print(len(test_idx))\n",
    "    print(name)\n",
    "    #use this line to control testing set\n",
    "    test_idx=np.array(range(N))\n",
    "    \n",
    "    train_mask = sample_mask(train_idx, y.shape[0])\n",
    "    val_mask = sample_mask(val_idx, y.shape[0])\n",
    "    print(\"test_idx\", test_idx)\n",
    "    print(\"y.shape[0]\", y.shape[0])\n",
    "    test_mask = sample_mask(test_idx, y.shape[0])# ?? why copy three times? First for center node, 2 for each metapath.\n",
    "    \n",
    "    y_train = np.zeros(y.shape)\n",
    "    y_val = np.zeros(y.shape)\n",
    "    y_test = np.zeros(y.shape)\n",
    "    y=y.values\n",
    "    y_train[train_mask, :] = y[train_mask, :]\n",
    "    y_val[val_mask, :] = y[val_mask, :]\n",
    "    y_test[test_mask, :] = y[test_mask, :]\n",
    "\n",
    "    # return selected_idx, selected_idx_2\n",
    "    print('y_train:{}, y_val:{}, y_test:{}, train_idx:{}, val_idx:{}, test_idx:{}'.format(y_train.shape,\n",
    "                                                                                          y_val.shape,\n",
    "                                                                                          y_test.shape,\n",
    "                                                                                          train_idx.shape,\n",
    "                                                                                          val_idx.shape,\n",
    "                                                                                          test_idx.shape))\n",
    "    truefeatures_list = [truefeatures, truefeatures]#, truefeatures, truefeatures]# ?? why copy three times? First for center node, 2 for each metapath.    \n",
    "    os.chdir(path_code)\n",
    "    return rownetworks, truefeatures_list, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(schools)):\n",
    "    adj_list, fea_list, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data_dblp(s=schools[i],st=schools_type[i])\n",
    "    \n",
    "\n",
    "    if featype == 'adj':\n",
    "        fea_list = adj_list\n",
    "\n",
    "\n",
    "\n",
    "    import scipy.sparse as sp\n",
    "\n",
    "    nb_nodes = fea_list[0].shape[0]\n",
    "    ft_size = fea_list[0].shape[1]\n",
    "    nb_classes = y_train.shape[1]\n",
    "    \n",
    "    fea_list = [fea[np.newaxis] for fea in fea_list]\n",
    "    adj_list = [adj[np.newaxis] for adj in adj_list]\n",
    "    y_train = y_train[np.newaxis]\n",
    "    y_val = y_val[np.newaxis]\n",
    "    y_test = y_test[np.newaxis]\n",
    "    train_mask = train_mask[np.newaxis]\n",
    "    val_mask = val_mask[np.newaxis]\n",
    "    test_mask = test_mask[np.newaxis]\n",
    "    print(\"s\")\n",
    "    \n",
    "    biases_list = [process.adj_to_bias(adj, [nb_nodes], nhood=1) for adj in adj_list]\n",
    "    name=schools[i]+'adj.p'\n",
    "    os.chdir(path_data)\n",
    "    pickle.dump(biases_list,open(name,\"wb\"))\n",
    "    #biases_list=pickle.load(open(name,\"rb\"))\n",
    "    \n",
    "    \n",
    "    print('build graph...')\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.name_scope('input'):\n",
    "            ftr_in_list = [tf.placeholder(dtype=tf.float32,\n",
    "                                          shape=(batch_size, nb_nodes, ft_size),\n",
    "                                          name='ftr_in_{}'.format(i))\n",
    "                           for i in range(len(fea_list))]\n",
    "            bias_in_list = [tf.placeholder(dtype=tf.float32,\n",
    "                                           shape=(batch_size, nb_nodes, nb_nodes),\n",
    "                                           name='bias_in_{}'.format(i))\n",
    "                            for i in range(len(biases_list))]\n",
    "            lbl_in = tf.placeholder(dtype=tf.int32, shape=(\n",
    "                batch_size, nb_nodes, nb_classes), name='lbl_in')\n",
    "            msk_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes),\n",
    "                                    name='msk_in')\n",
    "            attn_drop = tf.placeholder(dtype=tf.float32, shape=(), name='attn_drop')\n",
    "            ffd_drop = tf.placeholder(dtype=tf.float32, shape=(), name='ffd_drop')\n",
    "            is_train = tf.placeholder(dtype=tf.bool, shape=(), name='is_train')\n",
    "        # forward\n",
    "        logits, final_embedding, att_val = model.inference(ftr_in_list, nb_classes, nb_nodes, is_train,attn_drop, ffd_drop,bias_mat_list=bias_in_list,hid_units=hid_units, n_heads=n_heads,residual=residual, activation=nonlinearity)\n",
    "    \n",
    "        # cal masked_loss\n",
    "        log_resh = tf.reshape(logits, [-1, nb_classes])\n",
    "        lab_resh = tf.reshape(lbl_in, [-1, nb_classes])\n",
    "        msk_resh = tf.reshape(msk_in, [-1])\n",
    "        loss = model.masked_softmax_cross_entropy(log_resh, lab_resh, msk_resh)\n",
    "        accuracy = model.masked_accuracy(log_resh, lab_resh, msk_resh)\n",
    "        # optimzie\n",
    "        train_op = model.training(loss, lr, l2_coef)\n",
    "        vlss_mn = np.inf\n",
    "        vacc_mx = 0.0\n",
    "        curr_step = 0\n",
    "        saver = tf.train.Saver()\n",
    "        init_op = tf.group(tf.global_variables_initializer(),\n",
    "                           tf.local_variables_initializer())\n",
    "        with tf.Session(config=config) as sess:\n",
    "            #sess.run(init_op)\n",
    "\n",
    "            train_loss_avg = 0\n",
    "            train_acc_avg = 0\n",
    "            val_loss_avg = 0\n",
    "            val_acc_avg = 0\n",
    "            os.chdir(path_code)\n",
    "            saver.restore(sess, path_result+\"acm_allMP_multi_fea_.ckpt\")\n",
    "            print(\"Model restored.\")\n",
    "            \n",
    "            ts_size = fea_list[0].shape[0]\n",
    "            print(fea_list[0].shape)\n",
    "            ts_step = 0\n",
    "            ts_loss = 0.0\n",
    "            ts_acc = 0.0\n",
    "            print(\"test start\")\n",
    "            while ts_step * batch_size < ts_size:\n",
    "                print(ts_step)\n",
    "                print(batch_size)\n",
    "                print(ts_size)\n",
    "                # fd1 = {ftr_in: features[ts_step * batch_size:(ts_step + 1) * batch_size]}\n",
    "                fd1 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                       for i, d in zip(ftr_in_list, fea_list)}\n",
    "                fd2 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                       for i, d in zip(bias_in_list, biases_list)}\n",
    "                fd3 = {lbl_in: y_test[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "                       msk_in: test_mask[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "                \n",
    "                       is_train: False,\n",
    "                       attn_drop: 0.0,\n",
    "                       ffd_drop: 0.0}\n",
    "            \n",
    "                fd = fd1\n",
    "                fd.update(fd2)\n",
    "                fd.update(fd3)\n",
    "                \n",
    "                loss_value_ts, acc_ts, jhy_final_embedding, predicted = sess.run([loss, accuracy, final_embedding, log_resh],\n",
    "                                                                      feed_dict=fd)\n",
    "                import pickle\n",
    "                name=schools[i]+'.p'\n",
    "                os.chdir(path_result)\n",
    "                pickle.dump(predicted,open(name,\"wb\"))\n",
    "                ts_loss += loss_value_ts\n",
    "                ts_acc += acc_ts\n",
    "                ts_step += 1\n",
    "\n",
    "            print('Test loss:', ts_loss / ts_step,\n",
    "                  '; Test accuracy:', ts_acc / ts_step)\n",
    "    \n",
    "            \n",
    "            sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "opponent-november",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (147333720.py, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\yunfa\\AppData\\Local\\Temp\\ipykernel_10016\\147333720.py\"\u001b[1;36m, line \u001b[1;32m28\u001b[0m\n\u001b[1;33m    import scipy import sparse\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#run first block before running this one\n",
    "#get the testing result WITH submission data, i.e., the graph that this block contains comments and submission\n",
    "#the printed output is meaningless\n",
    "#predicted results are saved to .p file\n",
    "#prediction in this step is used in step 5, but not in step 4\n",
    "#if you have memory error, calculate \"biases_list\" variable for each school and save it locally. The code for this is commented out and you can use it directly\n",
    "\n",
    "#The feature, index, adjacency, label data are all different from the one used in previou block, contact the author to get these two sets of data\n",
    "\n",
    "path_result='C:\\\\Users\\\\yunfa\\\\dsc180b\\\\results\\\\'\n",
    "path_data=\"C:\\\\Users\\\\yunfa\\\\dsc180b\\\\data\"\n",
    "path_code=\"C:\\\\Users\\\\yunfa\\\\dsc180b\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.chdir(path_code)\n",
    "from gat import GAT, HeteGAT, HeteGAT_multi\n",
    "import process\n",
    "import importlib\n",
    "from layers import attn_head, SimpleAttLayer\n",
    "importlib.reload(process)\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy.sparse\n",
    "from scipy.sparse import csc_matrix\n",
    "import scipy import sparse\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import csv \n",
    "    \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "dataset = 'acm'\n",
    "featype = 'fea'\n",
    "# training params\n",
    "\n",
    "checkpt_file=path_result+'acm_allMP_multi_fea_.ckpt'\n",
    "\n",
    "\n",
    "print('Dataset: ' + dataset)\n",
    "print('----- Opt. hyperparams -----')\n",
    "\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "#Removed schools = \"dartmouth\"\n",
    "schools = [\"notredame2020\",\"uofm2019\",\"uofm2020\",\"columbia2019\",\"columbia2020\",\"UCSD2019\",\"UCSD2020\",\"berkeley2019\",\"berkeley2020\",\"Harvard2019\",\"Harvard2020\",\"ucla2019\",\"ucla2020\"]\n",
    "schools_type = [\"notredame2020\",\"uofm2019\",\"uofm2020\",\"columbia2019\",\"columbia2020\",\"UCSD2019\",\"UCSD2020\",\n",
    "           \"berkeley2019\",\"berkeley2020\",\"Harvard2019\",\"Harvard2020\",\"ucla2019\",\"ucla2020\"]\n",
    "\n",
    "\n",
    "#path_data='C:\\\\data\\\\HAN-modified\\\\tranfer learning\\\\'\n",
    "def load_data_dblp(s,path=path_data):\n",
    "    t=\"2019\"\n",
    "    \n",
    "    os.chdir(path_temp)\n",
    "    \n",
    "    name=\"feature_\"+s+t+\".csv\"\n",
    "    truefeatures=pd.read_csv(name,skip_blank_lines=True,header=None).values\n",
    "    N=truefeatures.shape[0]\n",
    "    \n",
    "    os.chdir(path_data)\n",
    "    \n",
    "    name=\"label_emt3_\"+s+t+\"_cm.csv\"\n",
    "    truelabels=pd.read_csv(name,skip_blank_lines=True,header=None)\n",
    "    \n",
    "    name=\"CCsym_\"+s+t+\".npz\"\n",
    "    dat_cc=scipy.sparse.load_npz(name);dat_cc=dat_cc.toarray();\n",
    "    rownetworks = [(dat_cc)]\n",
    "    \n",
    "    #TODO: dartmouth change to 2019, missing 2020\n",
    "    os.chdir(path_data)\n",
    "\n",
    "    y=truelabels    \n",
    "    name=\"train_index_\"+s+t+\"_cm.p\"\n",
    "    train_idx = pickle.load(open(name,\"rb\"));train_idx=np.array(train_idx)\n",
    "    name=\"to_be_labeled_index_\"+s+t+\"_cm.p\"\n",
    "    val_idx = pickle.load(open(name,\"rb\"));val_idx=np.array(val_idx)\n",
    "    name=\"test_index_\"+s+t+\"_cm.p\"\n",
    "    #name=\"test_index_\"+s+y+\"_cm.p\"#use this to do 3 class CV\n",
    "    test_idx = pickle.load(open(name,\"rb\"));test_idx=np.array(test_idx)\n",
    "    test_idx=np.array(range(N))\n",
    "    print(N)\n",
    "    \n",
    "    train_mask = sample_mask(train_idx, y.shape[0])\n",
    "    val_mask = sample_mask(val_idx, y.shape[0])\n",
    "    test_mask = sample_mask(test_idx, y.shape[0])# ?? why copy three times? First for center node, 2 for each metapath.\n",
    "    \n",
    "    y_train = np.zeros(y.shape)\n",
    "    y_val = np.zeros(y.shape)\n",
    "    y_test = np.zeros(y.shape)\n",
    "    y=y.values\n",
    "    y_train[train_mask, :] = y[train_mask, :]\n",
    "    y_val[val_mask, :] = y[val_mask, :]\n",
    "    y_test[test_mask, :] = y[test_mask, :]\n",
    "\n",
    "    # return selected_idx, selected_idx_2\n",
    "    print('y_train:{}, y_val:{}, y_test:{}, train_idx:{}, val_idx:{}, test_idx:{}'.format(y_train.shape,\n",
    "                                                                                          y_val.shape,\n",
    "                                                                                          y_test.shape,\n",
    "                                                                                          train_idx.shape,\n",
    "                                                                                          val_idx.shape,\n",
    "                                                                                          test_idx.shape))\n",
    "    truefeatures_list = [truefeatures, truefeatures]#, truefeatures, truefeatures]# ?? why copy three times? First for center node, 2 for each metapath.    \n",
    "    os.chdir(path_code)\n",
    "    return rownetworks, truefeatures_list, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(schools)):\n",
    "    adj_list, fea_list, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data_dblp(s=schools[i])\n",
    "    if featype == 'adj':\n",
    "        fea_list = adj_list\n",
    "\n",
    "    nb_nodes = fea_list[0].shape[0]\n",
    "    ft_size = fea_list[0].shape[1]\n",
    "    nb_classes = y_train.shape[1]\n",
    "\n",
    "    fea_list = [fea[np.newaxis] for fea in fea_list]\n",
    "    adj_list = [adj[np.newaxis] for adj in adj_list]\n",
    "    y_train = y_train[np.newaxis]\n",
    "    y_val = y_val[np.newaxis]\n",
    "    y_test = y_test[np.newaxis]\n",
    "    train_mask = train_mask[np.newaxis]\n",
    "    val_mask = val_mask[np.newaxis]\n",
    "    test_mask = test_mask[np.newaxis]\n",
    "    print(\"s\")\n",
    "    \n",
    "    biases_list = [process.adj_to_bias(adj, [nb_nodes], nhood=1) for adj in adj_list]\n",
    "    import pickle\n",
    "    name=schools[i]+'adj.p'\n",
    "    os.chdir(path_data)\n",
    "    pickle.dump(biases_list,open(name,\"wb\"))\n",
    "    #biases_list=pickle.load(open(name,\"rb\"))\n",
    "    \n",
    "    print('build graph...')\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.name_scope('input'):\n",
    "            ftr_in_list = [tf.placeholder(dtype=tf.float32,\n",
    "                                          shape=(batch_size, nb_nodes, ft_size),\n",
    "                                          name='ftr_in_{}'.format(i))\n",
    "                           for i in range(len(fea_list))]\n",
    "            bias_in_list = [tf.placeholder(dtype=tf.float32,\n",
    "                                           shape=(batch_size, nb_nodes, nb_nodes),\n",
    "                                           name='bias_in_{}'.format(i))\n",
    "                            for i in range(len(biases_list))]\n",
    "            lbl_in = tf.placeholder(dtype=tf.int32, shape=(\n",
    "                batch_size, nb_nodes, nb_classes), name='lbl_in')\n",
    "            msk_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes),\n",
    "                                    name='msk_in')\n",
    "            attn_drop = tf.placeholder(dtype=tf.float32, shape=(), name='attn_drop')\n",
    "            ffd_drop = tf.placeholder(dtype=tf.float32, shape=(), name='ffd_drop')\n",
    "            is_train = tf.placeholder(dtype=tf.bool, shape=(), name='is_train')\n",
    "        # forward\n",
    "        logits, final_embedding, att_val = model.inference(ftr_in_list, nb_classes, nb_nodes, is_train,attn_drop, ffd_drop,bias_mat_list=bias_in_list,hid_units=hid_units, n_heads=n_heads,residual=residual, activation=nonlinearity)\n",
    "    \n",
    "        # cal masked_loss\n",
    "        log_resh = tf.reshape(logits, [-1, nb_classes])\n",
    "        lab_resh = tf.reshape(lbl_in, [-1, nb_classes])\n",
    "        msk_resh = tf.reshape(msk_in, [-1])\n",
    "        loss = model.masked_softmax_cross_entropy(log_resh, lab_resh, msk_resh)\n",
    "        accuracy = model.masked_accuracy(log_resh, lab_resh, msk_resh)\n",
    "        # optimzie\n",
    "        train_op = model.training(loss, lr, l2_coef)\n",
    "        vlss_mn = np.inf\n",
    "        vacc_mx = 0.0\n",
    "        curr_step = 0\n",
    "        saver = tf.train.Saver()\n",
    "        init_op = tf.group(tf.global_variables_initializer(),\n",
    "                           tf.local_variables_initializer())\n",
    "        with tf.Session(config=config) as sess:\n",
    "            #sess.run(init_op)\n",
    "\n",
    "            train_loss_avg = 0\n",
    "            train_acc_avg = 0\n",
    "            val_loss_avg = 0\n",
    "            val_acc_avg = 0\n",
    "            os.chdir(path_code)\n",
    "            saver.restore(sess, path_result+\"acm_allMP_multi_fea_.ckpt\")\n",
    "            print(\"Model restored.\")\n",
    "            \n",
    "            ts_size = fea_list[0].shape[0]\n",
    "            print(fea_list[0].shape)\n",
    "            ts_step = 0\n",
    "            ts_loss = 0.0\n",
    "            ts_acc = 0.0\n",
    "            print(\"test start\")\n",
    "            while ts_step * batch_size < ts_size:\n",
    "                print(ts_step)\n",
    "                print(batch_size)\n",
    "                print(ts_size)\n",
    "                # fd1 = {ftr_in: features[ts_step * batch_size:(ts_step + 1) * batch_size]}\n",
    "                fd1 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                       for i, d in zip(ftr_in_list, fea_list)}\n",
    "                fd2 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                       for i, d in zip(bias_in_list, biases_list)}\n",
    "                fd3 = {lbl_in: y_test[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "                       msk_in: test_mask[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "                \n",
    "                       is_train: False,\n",
    "                       attn_drop: 0.0,\n",
    "                       ffd_drop: 0.0}\n",
    "                import psutil\n",
    "              \n",
    "                print(psutil.virtual_memory().available * 100 / psutil.virtual_memory().total)\n",
    "                fd = fd1\n",
    "                fd.update(fd2)\n",
    "                fd.update(fd3)\n",
    "                print('fini')\n",
    "                del bias_in_list\n",
    "                del biases_list\n",
    "                del ftr_in_list\n",
    "                del fea_list\n",
    "                print('dele finished')\n",
    "                print(psutil.virtual_memory().available * 100 / psutil.virtual_memory().total)\n",
    "                loss_value_ts, acc_ts, jhy_final_embedding, predicted = sess.run([loss, accuracy, final_embedding, log_resh],\n",
    "                                                                      feed_dict=fd)\n",
    "                import pickle\n",
    "                name=schools[i]+'.p'\n",
    "                os.chdir(path_result)\n",
    "                pickle.dump(predicted,open(name,\"wb\"))\n",
    "                ts_loss += loss_value_ts\n",
    "                ts_acc += acc_ts\n",
    "                ts_step += 1\n",
    "\n",
    "            print('Test loss:', ts_loss / ts_step,\n",
    "                  '; Test accuracy:', ts_acc / ts_step)\n",
    "    \n",
    "            \n",
    "            sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115701d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
