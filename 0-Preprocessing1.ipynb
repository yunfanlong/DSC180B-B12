{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "confirmed-discovery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[644 894 727]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preprocess Comment Graph\n",
    "\n",
    "Code Adapted from Alva Yan's Post-Covid Sentiment Analysis,\n",
    "https://github.com/AlvaYan/postCOVIDSentiAnalysis, and\n",
    "https://github.com/AlvaYan/Sentiment-Analysis-GNN-During-COVID19 with permission.\n",
    "\n",
    "Code was adapted while referencing public examples from the\n",
    "Keras documentation on GitHub:\n",
    "https://github.com/fchollet/keras/blob/master/examples\n",
    "'''\n",
    "\n",
    "\n",
    "#run this before running any subsequent block below\n",
    "#counting roughly how many pos/neg/neu are out there for combined Dartmouth set\n",
    "#Use this proportion to determine how much neutral class in \"non-neg\" class for testing dataset\n",
    "import numpy as np \n",
    "import os \n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "filepath2=\"C:\\\\Users\\\\yunfa\\\\dsc180b\\\\data\"\n",
    "\n",
    "types=[\"comment\"]\n",
    "years=['2019','2020']\n",
    "#years=['2020']\n",
    "schools = [\"notredame\",\"uofm\",\"columbia\",\"UCSD\",\"berkeley\",\"Harvard\",\"ucla\"]\n",
    "ct_thr=150\n",
    "\n",
    "#Positive: 1, negative:0\n",
    "#2020:1, 2019:0\n",
    "label_group=[\"label\",\"unlabel\"]\n",
    "months=['08','09','10','11']\n",
    "\n",
    "att_com=['time','id','body','Emotion','emo_pred']\n",
    "drop_com=[\"link_id\",'Topic',\"parent_id\",\"author_fullname\",\"author\",\"gildings\",\"score\",\"subreddit\",\"no_follow\",\"total_awards_received\",\"all_awardings\",\"is_submitter\",\"locked\",\"send_replies\",\"stickied\",]\n",
    "emo=[\"Very Positive\",\"Positive\",\"Neutral\",\"Negative\",\"Very negative\"]\n",
    "top=[\"Covid\",\"Academics\",\"Sports\",\"Campus/Students Life\",\"Social Media\",\"Religion\",\"Politics\",\"Others\"]\n",
    "\n",
    "#Positive: 1, negative:0\n",
    "num=[]\n",
    "for t in types:\n",
    "    for s in schools:\n",
    "        num_sch=[]\n",
    "        for y in years:\n",
    "            pos=0\n",
    "            neu=0\n",
    "            neg=0\n",
    "            for l in label_group:\n",
    "                f=s+t+y+l+\".csv\"\n",
    "                os.chdir(filepath2)\n",
    "                data=pd.read_csv(f,skip_blank_lines=True)\n",
    "                data=data[['body','Emotion']]\n",
    "                ct=0\n",
    "                for e in data['Emotion'].values:\n",
    "                    row=[0,0]\n",
    "                    if e=='Negative' or e=='Very negative':\n",
    "                        neg+=1\n",
    "                    elif e=='Positive' or e=='Very Positive':\n",
    "                        pos+=1\n",
    "                    elif e=='Neutral':\n",
    "                        neu+=1\n",
    "                    ct+=1\n",
    "                    if ct>ct_thr:\n",
    "                        break\n",
    "            num_sch.append([neg,neu,pos])\n",
    "        num.append(num_sch)\n",
    "                \n",
    "                \n",
    "s='dartmouth'\n",
    "f1='dartmouthcomment2019label.csv'\n",
    "f2='dartmouthcomment2019unlabel.csv'\n",
    "f3='dartmouthcomment2020label.csv'\n",
    "f4='dartmouthcomment2020unlabel.csv'\n",
    "os.chdir(filepath2)\n",
    "data1=pd.read_csv(f1,skip_blank_lines=True)\n",
    "# data1=data1.drop(drop_com, axis=1)\n",
    "data2=pd.read_csv(f2,skip_blank_lines=True)\n",
    "# data2=data2.drop(drop_com, axis=1)\n",
    "data3=pd.read_csv(f3,skip_blank_lines=True)\n",
    "# data3=data3.drop(drop_com, axis=1)\n",
    "data4=pd.read_csv(f4,skip_blank_lines=True)\n",
    "# data4=data4.drop(drop_com, axis=1)\n",
    "data = pd.concat([data1,data2,data3,data4], axis=0).reset_index(drop=True)\n",
    "data=data[['body','Emotion']]\n",
    "pos=0\n",
    "neu=0\n",
    "neg=0\n",
    "ct=0\n",
    "for e in data['Emotion'].values:\n",
    "    if e=='Negative' or e=='Very negative':\n",
    "        neg+=1\n",
    "    elif e=='Positive' or e=='Very Positive':\n",
    "        pos+=1\n",
    "    elif e=='Neutral':\n",
    "        neu+=1\n",
    "    ct+=1\n",
    "    if ct>ct_thr:\n",
    "        break\n",
    "num_sch=[]\n",
    "num_sch.append([0,0,0])\n",
    "num_sch.append([neg,neu,pos])\n",
    "num.append(num_sch)\n",
    "\n",
    "prop=sum(sum(np.array(num)))\n",
    "neu2neg_neu=prop[1]/(prop[0]+prop[1])\n",
    "neu2pos_neu=prop[1]/(prop[2]+prop[1])\n",
    "print(prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hydraulic-bahrain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dartmouth600\n",
      "dartmouth1200\n",
      "dartmouth1800\n",
      "dartmouth2400\n",
      "notredame600\n",
      "notredame1200\n",
      "notredame600\n",
      "notredame1200\n",
      "notredame1800\n",
      "notredame2400\n",
      "notredame3000\n",
      "notredame3600\n",
      "notredame4200\n",
      "notredame4800\n",
      "notredame5400\n",
      "notredame6000\n",
      "notredame6600\n",
      "notredame7200\n",
      "notredame7800\n",
      "uofm600\n",
      "uofm1200\n",
      "uofm1800\n",
      "uofm2400\n",
      "uofm3000\n",
      "uofm3600\n",
      "uofm4200\n",
      "uofm4800\n",
      "uofm5400\n",
      "uofm6000\n",
      "uofm6600\n",
      "uofm7200\n",
      "uofm7800\n",
      "uofm8400\n",
      "uofm9000\n",
      "uofm9600\n",
      "uofm10200\n",
      "uofm10800\n",
      "uofm11400\n",
      "uofm12000\n",
      "uofm600\n",
      "uofm1200\n",
      "uofm1800\n",
      "uofm2400\n",
      "uofm3000\n",
      "uofm3600\n",
      "uofm4200\n",
      "uofm4800\n",
      "uofm5400\n",
      "uofm6000\n",
      "uofm6600\n",
      "uofm7200\n",
      "uofm7800\n",
      "uofm8400\n",
      "uofm9000\n",
      "uofm9600\n",
      "uofm10200\n",
      "uofm10800\n",
      "uofm11400\n",
      "uofm12000\n",
      "uofm12600\n",
      "uofm13200\n",
      "uofm13800\n",
      "uofm14400\n",
      "columbia600\n",
      "columbia1200\n",
      "columbia1800\n",
      "columbia2400\n",
      "columbia3000\n",
      "columbia3600\n",
      "columbia4200\n",
      "columbia4800\n",
      "columbia600\n",
      "columbia1200\n",
      "columbia1800\n",
      "columbia2400\n",
      "columbia3000\n",
      "columbia3600\n",
      "columbia4200\n",
      "columbia4800\n",
      "columbia5400\n",
      "columbia6000\n",
      "columbia6600\n",
      "columbia7200\n",
      "columbia7800\n",
      "columbia8400\n",
      "columbia9000\n",
      "UCSD600\n",
      "UCSD1200\n",
      "UCSD1800\n",
      "UCSD2400\n",
      "UCSD3000\n",
      "UCSD3600\n",
      "UCSD4200\n",
      "UCSD4800\n",
      "UCSD5400\n",
      "UCSD6000\n",
      "UCSD6600\n",
      "UCSD7200\n",
      "UCSD7800\n",
      "UCSD8400\n",
      "UCSD9000\n",
      "UCSD9600\n",
      "UCSD10200\n",
      "UCSD10800\n",
      "UCSD11400\n",
      "UCSD12000\n",
      "UCSD12600\n",
      "UCSD13200\n",
      "UCSD13800\n",
      "UCSD14400\n",
      "UCSD15000\n",
      "UCSD15600\n",
      "UCSD16200\n",
      "UCSD16800\n",
      "UCSD17400\n",
      "UCSD600\n",
      "UCSD1200\n",
      "UCSD1800\n",
      "UCSD2400\n",
      "UCSD3000\n",
      "UCSD3600\n",
      "UCSD4200\n",
      "UCSD4800\n",
      "UCSD5400\n",
      "UCSD6000\n",
      "UCSD6600\n",
      "UCSD7200\n",
      "UCSD7800\n",
      "UCSD8400\n",
      "UCSD9000\n",
      "UCSD9600\n",
      "UCSD10200\n",
      "UCSD10800\n",
      "UCSD11400\n",
      "UCSD12000\n",
      "UCSD12600\n",
      "UCSD13200\n",
      "UCSD13800\n",
      "UCSD14400\n",
      "UCSD15000\n",
      "UCSD15600\n",
      "UCSD16200\n",
      "UCSD16800\n",
      "berkeley600\n",
      "berkeley1200\n",
      "berkeley1800\n",
      "berkeley2400\n",
      "berkeley3000\n",
      "berkeley3600\n",
      "berkeley4200\n",
      "berkeley4800\n",
      "berkeley5400\n",
      "berkeley6000\n",
      "berkeley6600\n",
      "berkeley7200\n",
      "berkeley7800\n",
      "berkeley8400\n",
      "berkeley9000\n",
      "berkeley9600\n",
      "berkeley10200\n",
      "berkeley10800\n",
      "berkeley11400\n",
      "berkeley12000\n",
      "berkeley12600\n",
      "berkeley13200\n",
      "berkeley13800\n",
      "berkeley14400\n",
      "berkeley15000\n",
      "berkeley15600\n",
      "berkeley16200\n",
      "berkeley600\n",
      "berkeley1200\n",
      "berkeley1800\n",
      "berkeley2400\n",
      "berkeley3000\n",
      "berkeley3600\n",
      "berkeley4200\n",
      "berkeley4800\n",
      "berkeley5400\n",
      "berkeley6000\n",
      "berkeley6600\n",
      "berkeley7200\n",
      "berkeley7800\n",
      "berkeley8400\n",
      "berkeley9000\n",
      "berkeley9600\n",
      "berkeley10200\n",
      "berkeley10800\n",
      "berkeley11400\n",
      "berkeley12000\n",
      "berkeley12600\n",
      "berkeley13200\n",
      "berkeley13800\n",
      "berkeley14400\n",
      "berkeley15000\n",
      "berkeley15600\n",
      "berkeley16200\n",
      "berkeley16800\n",
      "berkeley17400\n",
      "berkeley18000\n",
      "Harvard600\n",
      "Harvard1200\n",
      "Harvard1800\n",
      "Harvard2400\n",
      "Harvard600\n",
      "Harvard1200\n",
      "Harvard1800\n",
      "Harvard2400\n",
      "Harvard3000\n",
      "ucla600\n",
      "ucla1200\n",
      "ucla1800\n",
      "ucla2400\n",
      "ucla3000\n",
      "ucla3600\n",
      "ucla4200\n",
      "ucla4800\n",
      "ucla5400\n",
      "ucla6000\n",
      "ucla6600\n",
      "ucla7200\n",
      "ucla7800\n",
      "ucla8400\n",
      "ucla9000\n",
      "ucla9600\n",
      "ucla10200\n",
      "ucla10800\n",
      "ucla11400\n",
      "ucla12000\n",
      "ucla12600\n",
      "ucla13200\n",
      "ucla13800\n",
      "ucla14400\n",
      "ucla15000\n",
      "ucla15600\n",
      "ucla16200\n",
      "ucla16800\n",
      "ucla600\n",
      "ucla1200\n",
      "ucla1800\n",
      "ucla2400\n",
      "ucla3000\n",
      "ucla3600\n",
      "ucla4200\n",
      "ucla4800\n",
      "ucla5400\n",
      "ucla6000\n",
      "ucla6600\n",
      "ucla7200\n",
      "ucla7800\n",
      "ucla8400\n",
      "ucla9000\n",
      "ucla9600\n",
      "ucla10200\n",
      "ucla10800\n",
      "ucla11400\n",
      "ucla12000\n",
      "ucla12600\n",
      "ucla13200\n",
      "ucla13800\n",
      "ucla14400\n",
      "ucla15000\n",
      "ucla15600\n",
      "ucla16200\n",
      "ucla16800\n",
      "ucla17400\n",
      "ucla18000\n",
      "ucla18600\n"
     ]
    }
   ],
   "source": [
    "#Formualte train/test set by formulating sets of indices\n",
    "#For neg VS non-neg\n",
    "import numpy as np \n",
    "import os \n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "\n",
    "import math\n",
    "#stopword = stopwords.words(‘english’)\n",
    "import warnings\n",
    "import random\n",
    "import scipy.sparse\n",
    "\n",
    "# path_result='C:\\\\Backup of covid project\\\\2cls_negVSnonneg\\\\results\\\\'\n",
    "path_data=\"C:\\\\Users\\\\yunfa\\\\dsc180b\\\\data\"\n",
    "# path_code=\"C:\\\\Backup of covid project\\\\\"\n",
    "\n",
    "random.seed(10)\n",
    "filepath=path_data\n",
    "path=path_data\n",
    "os.chdir(filepath)\n",
    "\n",
    "types=[\"comment\"]\n",
    "years=['2019','2020']\n",
    "\n",
    "label_group=[\"label\",\"unlabel\"]\n",
    "months=['08','09','10','11']\n",
    "\n",
    "drop_com=[\"link_id\",\"parent_id\",\"author_fullname\",\"author\",\"gildings\",\"score\",\"subreddit\",\"no_follow\",\"total_awards_received\",\"all_awardings\",\"is_submitter\",\"locked\",\"send_replies\",\"stickied\",]\n",
    "att_com=[\"time\",\"id\",\"body\",\"Emotion\",\"Topic\"]\n",
    "emo=[\"Very negative\",\"Negative\",\"Neutral\",\"Positive\",\"Very Positive\"]\n",
    "top=[\"Covid\",\"Academics\",\"Sports\",\"Campus/Students Life\",\"Social Media\",\"Religion\",\"Politics\",\"Others\"]\n",
    "\n",
    "#FIRST DEAL WITH DARTMOUTH DATA\n",
    "#schools = [\"notredame\",\"uofm\",\"columbia\",\"dartmouth\",\"UCSD\",\"berkeley\",\"Harvard\",\"ucla\"]\n",
    "schools = [\"dartmouth\"]\n",
    "for t in types:\n",
    "    for s in schools:\n",
    "        train_index=[]\n",
    "        test_index=[]\n",
    "        to_be_labeled_index=[]\n",
    "        idx=0\n",
    "        \n",
    "        n_neu=np.round(50*neu2pos_neu)\n",
    "        n_test=[50,n_neu,50-n_neu]\n",
    "        for y in years:\n",
    "            for l in label_group:\n",
    "                f=s+t+y+l+\".csv\"\n",
    "                os.chdir(filepath)\n",
    "                data=pd.read_csv(f,skip_blank_lines=True)\n",
    "                # data=data.drop(drop_com, axis=1)\n",
    "                for d in range(len(data['id'])):\n",
    "                    if str(data['time'][d])[4:6] in months:\n",
    "                        if isinstance(data['Emotion'][d],str)==False:\n",
    "                            to_be_labeled_index.append(idx)\n",
    "                        elif data['Emotion'][d]==emo[0] and n_test[0]>0:\n",
    "                            test_index.append(idx)\n",
    "                            n_test[0]=n_test[0]-1\n",
    "                        elif data['Emotion'][d]==emo[0]:\n",
    "                            train_index.append(idx)\n",
    "                        elif data['Emotion'][d]==emo[1] and n_test[0]>0:\n",
    "                            test_index.append(idx)\n",
    "                            n_test[0]=n_test[0]-1\n",
    "                        elif data['Emotion'][d]==emo[1]:\n",
    "                            train_index.append(idx)\n",
    "                        elif data['Emotion'][d]==emo[2] and n_test[1]>0:\n",
    "                            test_index.append(idx)\n",
    "                            n_test[1]=n_test[1]-1\n",
    "                        elif data['Emotion'][d]==emo[2]:\n",
    "                            train_index.append(idx)\n",
    "                        elif data['Emotion'][d]==emo[3] and n_test[2]>0:\n",
    "                            test_index.append(idx)\n",
    "                            n_test[2]=n_test[2]-1\n",
    "                        elif data['Emotion'][d]==emo[3]:\n",
    "                            train_index.append(idx)\n",
    "                        elif data['Emotion'][d]==emo[4] and n_test[2]>0:\n",
    "                            test_index.append(idx)\n",
    "                            n_test[2]=n_test[2]-1\n",
    "                        elif data['Emotion'][d]==emo[4]:\n",
    "                            train_index.append(idx)\n",
    "                        else:\n",
    "                            to_be_labeled_index.append(idx)\n",
    "            \n",
    "                        os.chdir(path)  \n",
    "                        \n",
    "            \n",
    "                        idx=idx+1\n",
    "                        if idx/600-int(idx/600)==0:\n",
    "                            print(s+str(idx))    \n",
    "            os.chdir(path)\n",
    "            name=\"train_index_\"+s+y+\"_cm.p\"\n",
    "            pickle.dump(train_index,open(name,\"wb\"))\n",
    "            name=\"test_index_\"+s+y+\"_cm.p\"\n",
    "            pickle.dump(test_index,open(name,\"wb\"))\n",
    "            name=\"to_be_labeled_index_\"+s+y+\"_cm.p\"\n",
    "            pickle.dump(to_be_labeled_index,open(name,\"wb\"))\n",
    "        \n",
    "#deal with other schools\n",
    "schools = [\"notredame\",\"uofm\",\"columbia\",\"UCSD\",\"berkeley\",\"Harvard\",\"ucla\"]\n",
    "for t in types:\n",
    "    for s in schools:\n",
    "        for y in years:\n",
    "            train_index=[]\n",
    "            test_index=[]\n",
    "            to_be_labeled_index=[]\n",
    "            idx=0\n",
    "            n_neu=np.round(50*neu2pos_neu)\n",
    "            n_test=[50,n_neu,50-n_neu]\n",
    "            for l in label_group:\n",
    "                f=s+t+y+l+\".csv\"\n",
    "                os.chdir(filepath)\n",
    "                data=pd.read_csv(f,skip_blank_lines=True)\n",
    "                # data=data.drop(drop_com, axis=1)\n",
    "                for d in range(len(data['id'])):\n",
    "                    if str(data['time'][d])[4:6] in months:\n",
    "                        if isinstance(data['Emotion'][d],str)==False:\n",
    "                            to_be_labeled_index.append(idx)\n",
    "                        elif data['Emotion'][d]==emo[0] and n_test[0]>0:\n",
    "                            test_index.append(idx)\n",
    "                            n_test[0]=n_test[0]-1\n",
    "                        elif data['Emotion'][d]==emo[0]:\n",
    "                            train_index.append(idx)\n",
    "                        elif data['Emotion'][d]==emo[1] and n_test[0]>0:\n",
    "                            test_index.append(idx)\n",
    "                            n_test[0]=n_test[0]-1\n",
    "                        elif data['Emotion'][d]==emo[1]:\n",
    "                            train_index.append(idx)\n",
    "                        elif data['Emotion'][d]==emo[2] and n_test[1]>0:\n",
    "                            test_index.append(idx)\n",
    "                            n_test[1]=n_test[1]-1\n",
    "                        elif data['Emotion'][d]==emo[2]:\n",
    "                            to_be_labeled_index.append(idx)\n",
    "                        elif data['Emotion'][d]==emo[3] and n_test[2]>0:\n",
    "                            test_index.append(idx)\n",
    "                            n_test[2]=n_test[2]-1\n",
    "                        elif data['Emotion'][d]==emo[3]:\n",
    "                            train_index.append(idx)\n",
    "                        elif data['Emotion'][d]==emo[4] and n_test[2]>0:\n",
    "                            test_index.append(idx)\n",
    "                            n_test[2]=n_test[2]-1\n",
    "                        elif data['Emotion'][d]==emo[4]:\n",
    "                            train_index.append(idx)\n",
    "                        else:\n",
    "                            to_be_labeled_index.append(idx)\n",
    "            \n",
    "                        os.chdir(path)  \n",
    "                        \n",
    "            \n",
    "                        idx=idx+1\n",
    "                        if idx/600-int(idx/600)==0:\n",
    "                            print(s+str(idx))    \n",
    "            os.chdir(path)\n",
    "            name=\"train_index_\"+s+y+\"_cm.p\"\n",
    "            pickle.dump(train_index,open(name,\"wb\"))\n",
    "            name=\"test_index_\"+s+y+\"_cm.p\"\n",
    "            pickle.dump(test_index,open(name,\"wb\"))\n",
    "            name=\"to_be_labeled_index_\"+s+y+\"_cm.p\"\n",
    "            pickle.dump(to_be_labeled_index,open(name,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "changing-parts",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yunfa\\anaconda3\\envs\\ECE176\\lib\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (None)/charset_normalizer (3.1.0) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training numbers in DM: [94, 280, 321]\n",
      "testing numbers in DM: [50, 28, 22]\n",
      "testing numbers in notredame2019: [50, 28, 22]\n",
      "testing numbers in notredame2020: [50, 28, 22]\n",
      "testing numbers in uofm2019: [50, 28, 22]\n",
      "testing numbers in uofm2020: [50, 28, 22]\n",
      "testing numbers in columbia2019: [50, 28, 22]\n",
      "testing numbers in columbia2020: [50, 28, 22]\n",
      "testing numbers in UCSD2019: [50, 28, 22]\n",
      "testing numbers in UCSD2020: [50, 28, 22]\n",
      "testing numbers in berkeley2019: [50, 28, 22]\n",
      "testing numbers in berkeley2020: [50, 28, 22]\n",
      "testing numbers in Harvard2019: [50, 28, 22]\n",
      "testing numbers in Harvard2020: [50, 28, 22]\n",
      "testing numbers in ucla2019: [50, 28, 22]\n",
      "testing numbers in ucla2020: [50, 28, 22]\n"
     ]
    }
   ],
   "source": [
    "#counting how much training sample we have for DM set \n",
    "#and count ho much testing sample we have for each dataset\n",
    "#for neg VS non-neg\n",
    "import numpy as np \n",
    "import urllib.request \n",
    "import os \n",
    "import csv \n",
    "import requests \n",
    "import time\n",
    "import math\n",
    "import rando\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "import math\n",
    "#stopword = stopwords.words(‘english’)\n",
    "import warnings\n",
    "import random\n",
    "import scipy.sparse\n",
    "\n",
    "# path_result='C:\\\\Backup of covid project\\\\2cls_negVSnonneg\\\\results\\\\'\n",
    "path_data=\"C:\\\\Users\\\\yunfa\\\\dsc180b\\\\data\"\n",
    "# path_code=\"C:\\\\Backup of covid project\\\\\"\n",
    "\n",
    "random.seed(10)\n",
    "filepath=path_data\n",
    "path=path_data\n",
    "os.chdir(filepath)\n",
    "\n",
    "types=[\"comment\"]\n",
    "years=['2019','2020']\n",
    "\n",
    "label_group=[\"label\",\"unlabel\"]\n",
    "months=['08','09','10','11']\n",
    "\n",
    "drop_com=[\"link_id\",\"parent_id\",\"author_fullname\",\"author\",\"gildings\",\"score\",\"subreddit\",\"no_follow\",\"total_awards_received\",\"all_awardings\",\"is_submitter\",\"locked\",\"send_replies\",\"stickied\",]\n",
    "att_com=[\"time\",\"id\",\"body\",\"Emotion\",\"Topic\"]\n",
    "\n",
    "#emo=[\"Very Positive\",\"Positive\",\"Neutral\",\"Negative\",\"Very negative\"]\n",
    "emo=[\"Very negative\",\"Negative\",\"Neutral\",\"Positive\",\"Very Positive\"]\n",
    "top=[\"Covid\",\"Academics\",\"Sports\",\"Campus/Students Life\",\"Social Media\",\"Religion\",\"Politics\",\"Others\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FIRST DEAL WITH DARTMOUTH DATA\n",
    "schools = [\"dartmouth\"]\n",
    "for t in types:\n",
    "    for s in schools:\n",
    "        train_index=[]\n",
    "        test_index=[]\n",
    "        to_be_labeled_index=[]\n",
    "        idx=0\n",
    "        n_neu=np.round(50*neu2pos_neu)\n",
    "        n_test=[50,n_neu,50-n_neu]\n",
    "        train_neg=0\n",
    "        train_pos=0\n",
    "        train_neu=0\n",
    "        test_neg=0\n",
    "        test_pos=0\n",
    "        test_neu=0\n",
    "        for y in years:\n",
    "            for l in label_group:\n",
    "                f=s+t+y+l+\".csv\"\n",
    "                os.chdir(filepath)\n",
    "                data=pd.read_csv(f,skip_blank_lines=True)\n",
    "                # data=data.drop(drop_com, axis=1)\n",
    "                for d in range(len(data['id'])):\n",
    "                    if str(data['time'][d])[4:6] in months:\n",
    "                        if isinstance(data['Emotion'][d],str)==False:\n",
    "                            to_be_labeled_index.append(idx)\n",
    "                        elif data['Emotion'][d]==emo[0] and n_test[0]>0:\n",
    "                            test_index.append(idx)\n",
    "                            n_test[0]=n_test[0]-1\n",
    "                            test_neg+=1\n",
    "                        elif data['Emotion'][d]==emo[0]:\n",
    "                            train_index.append(idx)\n",
    "                            train_neg+=1\n",
    "                        elif data['Emotion'][d]==emo[1] and n_test[0]>0:\n",
    "                            test_index.append(idx)\n",
    "                            n_test[0]=n_test[0]-1\n",
    "                            test_neg+=1\n",
    "                        elif data['Emotion'][d]==emo[1]:\n",
    "                            train_index.append(idx)\n",
    "                            train_neg+=1\n",
    "                        elif data['Emotion'][d]==emo[2] and n_test[1]>0:\n",
    "                            test_index.append(idx)\n",
    "                            n_test[1]=n_test[1]-1\n",
    "                            test_neu+=1\n",
    "                        elif data['Emotion'][d]==emo[2]:\n",
    "                            train_index.append(idx)\n",
    "                            train_neu+=1\n",
    "                        elif data['Emotion'][d]==emo[3] and n_test[2]>0:\n",
    "                            test_index.append(idx)\n",
    "                            n_test[2]=n_test[2]-1\n",
    "                            test_pos+=1\n",
    "                        elif data['Emotion'][d]==emo[3]:\n",
    "                            train_index.append(idx)\n",
    "                            train_pos+=1\n",
    "                        elif data['Emotion'][d]==emo[4] and n_test[2]>0:\n",
    "                            test_index.append(idx)\n",
    "                            n_test[2]=n_test[2]-1\n",
    "                            test_pos+=1\n",
    "                        elif data['Emotion'][d]==emo[4]:\n",
    "                            train_index.append(idx)\n",
    "                            train_pos+=1\n",
    "                        else:\n",
    "                            to_be_labeled_index.append(idx)\n",
    "            \n",
    "                        os.chdir(path)  \n",
    "                        \n",
    "            \n",
    "                        idx=idx+1 \n",
    "        print('training numbers in DM: '+str([train_neg,train_neu,train_pos]))\n",
    "        print('testing numbers in DM: '+str([test_neg,test_neu,test_pos]))\n",
    "        \n",
    "#Deal with other schools\n",
    "schools = [\"notredame\",\"uofm\",\"columbia\",\"UCSD\",\"berkeley\",\"Harvard\",\"ucla\"]\n",
    "for t in types:\n",
    "    for s in schools:\n",
    "        for y in years:\n",
    "            train_index=[]\n",
    "            test_index=[]\n",
    "            to_be_labeled_index=[]\n",
    "            idx=0\n",
    "            n_neu=np.round(50*neu2pos_neu)\n",
    "            n_test=[50,n_neu,50-n_neu]\n",
    "            test_neg=0\n",
    "            test_pos=0\n",
    "            test_neu=0\n",
    "            for l in label_group:\n",
    "                f=s+t+y+l+\".csv\"\n",
    "                os.chdir(filepath)\n",
    "                data=pd.read_csv(f,skip_blank_lines=True)\n",
    "                # data=data.drop(drop_com, axis=1)\n",
    "                for d in range(len(data['id'])):\n",
    "                    if str(data['time'][d])[4:6] in months:\n",
    "                        if isinstance(data['Emotion'][d],str)==False:\n",
    "                            to_be_labeled_index.append(idx)\n",
    "                        elif data['Emotion'][d]==emo[0] and n_test[0]>0:\n",
    "                            test_index.append(idx)\n",
    "                            n_test[0]=n_test[0]-1\n",
    "                            test_neg+=1\n",
    "                        elif data['Emotion'][d]==emo[0]:\n",
    "                            to_be_labeled_index.append(idx)\n",
    "                        elif data['Emotion'][d]==emo[1] and n_test[0]>0:\n",
    "                            test_index.append(idx)\n",
    "                            n_test[0]=n_test[0]-1\n",
    "                            test_neg+=1\n",
    "                        elif data['Emotion'][d]==emo[1]:\n",
    "                            to_be_labeled_index.append(idx)\n",
    "                        elif data['Emotion'][d]==emo[2] and n_test[1]>0:\n",
    "                            test_index.append(idx)\n",
    "                            n_test[1]=n_test[1]-1\n",
    "                            test_neu+=1\n",
    "                        elif data['Emotion'][d]==emo[2]:\n",
    "                            to_be_labeled_index.append(idx)\n",
    "                        elif data['Emotion'][d]==emo[3] and n_test[2]>0:\n",
    "                            test_index.append(idx)\n",
    "                            n_test[2]=n_test[2]-1\n",
    "                            test_pos+=1\n",
    "                        elif data['Emotion'][d]==emo[3]:\n",
    "                            to_be_labeled_index.append(idx)\n",
    "                        elif data['Emotion'][d]==emo[4] and n_test[2]>0:\n",
    "                            test_index.append(idx)\n",
    "                            n_test[2]=n_test[2]-1\n",
    "                            test_pos+=1\n",
    "                        elif data['Emotion'][d]==emo[4]:\n",
    "                            to_be_labeled_index.append(idx)\n",
    "                        else:\n",
    "                            to_be_labeled_index.append(idx)\n",
    "            \n",
    "                        os.chdir(path)  \n",
    "                        \n",
    "            \n",
    "                        idx=idx+1\n",
    "                        #if idx/600-int(idx/600)==0:\n",
    "                        #    print(s+str(idx))    \n",
    "            print('testing numbers in '+s+y+\": \"+str([test_neg,test_neu,test_pos]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "superior-adoption",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save each sample's class using one hot encoding\n",
    "#For neg VS non-neg\n",
    "import numpy as np \n",
    "import os \n",
    "import csv \n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import warnings\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "#import tensorflow_hub as hub\n",
    "#import tensorflow_text\n",
    "\n",
    "filepath2=\"C:\\\\Users\\\\yunfa\\\\180b\\\\data\"\n",
    "\n",
    "path=\"C:\\\\Users\\\\yunfa\\\\180b\\\\data\"\n",
    "\n",
    "types=[\"comment\"]\n",
    "years=['2019','2020']\n",
    "#years=['2020']\n",
    "schools = [\"notredame\",\"uofm\",\"columbia\",\"UCSD\",\"berkeley\",\"Harvard\",\"ucla\",\"dartmouth\"]\n",
    "\n",
    "#2020:1, 2019:0\n",
    "label_group=[\"label\",\"unlabel\"]\n",
    "months=['08','09','10','11']\n",
    "\n",
    "att_com=['time','id','body','Emotion','emo_pred']\n",
    "drop_com=[\"link_id\",'Topic',\"parent_id\",\"author_fullname\",\"author\",\"gildings\",\"score\",\"subreddit\",\"no_follow\",\"total_awards_received\",\"all_awardings\",\"is_submitter\",\"locked\",\"send_replies\",\"stickied\",]\n",
    "emo=[\"Very Positive\",\"Positive\",\"Neutral\",\"Negative\",\"Very negative\"]\n",
    "top=[\"Covid\",\"Academics\",\"Sports\",\"Campus/Students Life\",\"Social Media\",\"Religion\",\"Politics\",\"Others\"]\n",
    "\n",
    "#non-Neg: 1, negative:0\n",
    "for t in types:\n",
    "    for s in schools:\n",
    "        for y in years:\n",
    "            for l in label_group:\n",
    "                f=s+t+y+l+\".csv\"\n",
    "                os.chdir(filepath2)\n",
    "                data=pd.read_csv(f,skip_blank_lines=True)\n",
    "                data=data[['body','Emotion']]\n",
    "\n",
    "                for e in data['Emotion'].values:\n",
    "                    row=[0,0]\n",
    "                    if e=='Negative' or e=='Very negative':\n",
    "                        row=[1,0]\n",
    "                    elif e=='Positive' or e=='Very Positive':\n",
    "                        row=[0,1]\n",
    "                    elif e=='Neutral':\n",
    "                        row=[0,1]\n",
    "                \n",
    "                    #turning testing_index to testing_index_2cls\n",
    "                    os.chdir(path)  \n",
    "                        \n",
    "                    name=\"label_emt3_\"+s+y+\"_cm.csv\"\n",
    "                    e=open(name, 'a',newline='')\n",
    "                    with e:\n",
    "                        writer = csv.writer(e, delimiter=',')\n",
    "                        writer.writerow(row)\n",
    "                \n",
    "s='dartmouth'\n",
    "f1='dartmouthcomment2019label.csv'\n",
    "f2='dartmouthcomment2019unlabel.csv'\n",
    "f3='dartmouthcomment2020label.csv'\n",
    "f4='dartmouthcomment2020unlabel.csv'\n",
    "os.chdir(filepath2)\n",
    "data1=pd.read_csv(f1,skip_blank_lines=True)\n",
    "# data1=data1.drop(drop_com, axis=1)\n",
    "data2=pd.read_csv(f2,skip_blank_lines=True)\n",
    "# data2=data2.drop(drop_com, axis=1)\n",
    "data3=pd.read_csv(f3,skip_blank_lines=True)\n",
    "# data3=data3.drop(drop_com, axis=1)\n",
    "data4=pd.read_csv(f4,skip_blank_lines=True)\n",
    "# data4=data4.drop(drop_com, axis=1)\n",
    "data = pd.concat([data1,data2,data3,data4], axis=0).reset_index(drop=True)\n",
    "data=data[['body','Emotion']]\n",
    "\n",
    "\n",
    "for e in data['Emotion'].values:\n",
    "    row=[0,0]\n",
    "    if e=='Negative' or e=='Very negative':\n",
    "        row=[1,0]\n",
    "    elif e=='Positive' or e=='Very Positive':\n",
    "        row=[0,1]\n",
    "    elif e=='Neutral':\n",
    "        row=[0,1]\n",
    "                \n",
    "    #turning testing_index to testing_index_2cls\n",
    "    os.chdir(path)  \n",
    "                        \n",
    "    name=\"label_emt3_\"+s+\"_cm.csv\"\n",
    "    e=open(name, 'a',newline='')\n",
    "    with e:\n",
    "        writer = csv.writer(e, delimiter=',')\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
